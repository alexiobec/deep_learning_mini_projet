{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'position', 'fr_gb', 'gb_fr', 'fr_cwe', 'cwe_fr', 'fr_ch',\n",
      "       'ch_fr', 'fr_it', 'it_fr', 'fr_es', 'es_fr', 'export_france',\n",
      "       'import_france'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"imports-exports-commerciaux.csv\",sep=\";\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>position</th>\n",
       "      <th>fr_gb</th>\n",
       "      <th>gb_fr</th>\n",
       "      <th>fr_cwe</th>\n",
       "      <th>cwe_fr</th>\n",
       "      <th>fr_ch</th>\n",
       "      <th>ch_fr</th>\n",
       "      <th>fr_it</th>\n",
       "      <th>it_fr</th>\n",
       "      <th>fr_es</th>\n",
       "      <th>es_fr</th>\n",
       "      <th>export_france</th>\n",
       "      <th>import_france</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-09-15</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>-384.0</td>\n",
       "      <td>591.0</td>\n",
       "      <td>-2055.0</td>\n",
       "      <td>3017.0</td>\n",
       "      <td>-1898.0</td>\n",
       "      <td>2403.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>-568.0</td>\n",
       "      <td>6236.0</td>\n",
       "      <td>-4905.0</td>\n",
       "      <td>2010-09-15-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-09-15</td>\n",
       "      <td>12.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>-2253.0</td>\n",
       "      <td>3067.0</td>\n",
       "      <td>-2104.0</td>\n",
       "      <td>2404.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>-399.0</td>\n",
       "      <td>5786.0</td>\n",
       "      <td>-4881.0</td>\n",
       "      <td>2010-09-15-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-09-15</td>\n",
       "      <td>14.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-76.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>-2734.0</td>\n",
       "      <td>3046.0</td>\n",
       "      <td>-2052.0</td>\n",
       "      <td>2404.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>-361.0</td>\n",
       "      <td>6002.0</td>\n",
       "      <td>-5223.0</td>\n",
       "      <td>2010-09-15-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-09-15</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>-296.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>-2257.0</td>\n",
       "      <td>3067.0</td>\n",
       "      <td>-975.0</td>\n",
       "      <td>2404.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>-221.0</td>\n",
       "      <td>6032.0</td>\n",
       "      <td>-3749.0</td>\n",
       "      <td>2010-09-15-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-09-15</td>\n",
       "      <td>17.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>-305.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>-2468.0</td>\n",
       "      <td>3031.0</td>\n",
       "      <td>-656.0</td>\n",
       "      <td>2404.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>-263.0</td>\n",
       "      <td>6323.0</td>\n",
       "      <td>-3692.0</td>\n",
       "      <td>2010-09-15-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149011</th>\n",
       "      <td>2015-04-08</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2361.5</td>\n",
       "      <td>-3085.0</td>\n",
       "      <td>2896.2</td>\n",
       "      <td>-1028.8</td>\n",
       "      <td>1699.0</td>\n",
       "      <td>-36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-900.0</td>\n",
       "      <td>8956.0</td>\n",
       "      <td>-5049.0</td>\n",
       "      <td>2015-04-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149012</th>\n",
       "      <td>2015-04-08</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>2214.3</td>\n",
       "      <td>-3639.0</td>\n",
       "      <td>3241.5</td>\n",
       "      <td>-2196.5</td>\n",
       "      <td>783.0</td>\n",
       "      <td>-757.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>8237.0</td>\n",
       "      <td>-6708.0</td>\n",
       "      <td>2015-04-08-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149013</th>\n",
       "      <td>2015-04-08</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2398.6</td>\n",
       "      <td>-3032.0</td>\n",
       "      <td>3616.9</td>\n",
       "      <td>-1167.6</td>\n",
       "      <td>3175.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>11466.0</td>\n",
       "      <td>-4349.0</td>\n",
       "      <td>2015-04-08-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149014</th>\n",
       "      <td>2015-04-08</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3588.8</td>\n",
       "      <td>-3634.2</td>\n",
       "      <td>3951.0</td>\n",
       "      <td>-1967.8</td>\n",
       "      <td>3152.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>12691.0</td>\n",
       "      <td>-5710.0</td>\n",
       "      <td>2015-04-08-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149015</th>\n",
       "      <td>2015-04-08</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2342.1</td>\n",
       "      <td>-3290.0</td>\n",
       "      <td>2945.3</td>\n",
       "      <td>-1463.5</td>\n",
       "      <td>812.0</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>517.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>8616.0</td>\n",
       "      <td>-4933.0</td>\n",
       "      <td>2015-04-08-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149016 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date  position   fr_gb  gb_fr  fr_cwe  cwe_fr   fr_ch   ch_fr  \\\n",
       "0       2010-09-15      10.0    25.0 -384.0   591.0 -2055.0  3017.0 -1898.0   \n",
       "1       2010-09-15      12.0    25.0 -125.0   271.0 -2253.0  3067.0 -2104.0   \n",
       "2       2010-09-15      14.0    45.0  -76.0   304.0 -2734.0  3046.0 -2052.0   \n",
       "3       2010-09-15      16.0    25.0 -296.0   333.0 -2257.0  3067.0  -975.0   \n",
       "4       2010-09-15      17.0   293.0 -305.0   392.0 -2468.0  3031.0  -656.0   \n",
       "...            ...       ...     ...    ...     ...     ...     ...     ...   \n",
       "149011  2015-04-08       4.0  2000.0    0.0  2361.5 -3085.0  2896.2 -1028.8   \n",
       "149012  2015-04-08      11.0  1999.0  -16.0  2214.3 -3639.0  3241.5 -2196.5   \n",
       "149013  2015-04-08      19.0  2000.0    0.0  2398.6 -3032.0  3616.9 -1167.6   \n",
       "149014  2015-04-08      21.0  2000.0    0.0  3588.8 -3634.2  3951.0 -1967.8   \n",
       "149015  2015-04-08      24.0  2000.0    0.0  2342.1 -3290.0  2945.3 -1463.5   \n",
       "\n",
       "         fr_it  it_fr  fr_es  es_fr  export_france  import_france  \\\n",
       "0       2403.0    0.0  200.0 -568.0         6236.0        -4905.0   \n",
       "1       2404.0    0.0   19.0 -399.0         5786.0        -4881.0   \n",
       "2       2404.0    0.0  203.0 -361.0         6002.0        -5223.0   \n",
       "3       2404.0    0.0  203.0 -221.0         6032.0        -3749.0   \n",
       "4       2404.0    0.0  203.0 -263.0         6323.0        -3692.0   \n",
       "...        ...    ...    ...    ...            ...            ...   \n",
       "149011  1699.0  -36.0    0.0 -900.0         8956.0        -5049.0   \n",
       "149012   783.0 -757.0    0.0 -100.0         8237.0        -6708.0   \n",
       "149013  3175.0  -50.0  277.0 -100.0        11466.0        -4349.0   \n",
       "149014  3152.0   -9.0    0.0 -100.0        12691.0        -5710.0   \n",
       "149015   812.0  -80.0  517.0 -100.0         8616.0        -4933.0   \n",
       "\n",
       "            timestamp  \n",
       "0       2010-09-15-10  \n",
       "1       2010-09-15-12  \n",
       "2       2010-09-15-14  \n",
       "3       2010-09-15-16  \n",
       "4       2010-09-15-17  \n",
       "...               ...  \n",
       "149011  2015-04-08-04  \n",
       "149012  2015-04-08-11  \n",
       "149013  2015-04-08-19  \n",
       "149014  2015-04-08-21  \n",
       "149015  2015-04-08-24  \n",
       "\n",
       "[149016 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On reformate la date et l'heure en combinant les colonnes \"date\" et \"position\" (qui représente l'heure) dans une seule colonne timestamp\n",
    "from datetime import datetime\n",
    "def heure(n) :\n",
    "    if n < 10 :\n",
    "        return f\"0{int(n)}\"\n",
    "    else :\n",
    "        return str(int(n))\n",
    "\n",
    "df[\"timestamp\"] = df[\"date\"]+ \"-\"+ df[\"position\"].map(heure)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On enlève les colonnes qui ne nous intéressent pas, c'est à dire toutes celles concernant l'échange spécifique entre la france et un seul autre pays, étant donné que les colonnes imports et exports sont la somme de ces valeurs\n",
    "df = df.drop([\"fr_gb\",\"gb_fr\",\"fr_cwe\",\"cwe_fr\",\"fr_ch\",\"ch_fr\",\"fr_it\",\"it_fr\",\"fr_es\",\"es_fr\",\"date\",\"position\",\"import_france\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        export_france      timestamp\n",
      "87081         13024.0  2021-12-30-19\n",
      "111596        13459.0  2021-12-30-20\n",
      "86233         14033.0  2021-12-30-21\n",
      "32925         12965.0  2021-12-30-22\n",
      "144207        11875.0  2021-12-30-23\n",
      "87086         12004.0  2021-12-30-24\n",
      "87091         11131.0  2021-12-31-01\n",
      "111601        10888.0  2021-12-31-02\n",
      "32930         10836.0  2021-12-31-03\n",
      "111606        10681.0  2021-12-31-04\n",
      "111611        10340.0  2021-12-31-05\n",
      "32935         10162.0  2021-12-31-06\n",
      "86238         10230.0  2021-12-31-07\n",
      "87096         10499.0  2021-12-31-08\n",
      "32940         11210.0  2021-12-31-09\n",
      "111616        10328.0  2021-12-31-10\n",
      "87101          9919.0  2021-12-31-11\n",
      "87106         11022.0  2021-12-31-12\n",
      "111621        11106.0  2021-12-31-13\n",
      "144212        11456.0  2021-12-31-14\n",
      "86243          9635.0  2021-12-31-15\n",
      "87111          9554.0  2021-12-31-16\n",
      "111626         8474.0  2021-12-31-17\n",
      "87116          8742.0  2021-12-31-18\n",
      "86248         10430.0  2021-12-31-19\n",
      "32945         10484.0  2021-12-31-20\n",
      "32950         11895.0  2021-12-31-21\n",
      "86253         12731.0  2021-12-31-22\n",
      "87121         10313.0  2021-12-31-23\n",
      "32955          9666.0  2021-12-31-24\n"
     ]
    }
   ],
   "source": [
    "df = df.sort_values(by=\"timestamp\")\n",
    "print(df[-30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a des doublons dans les dates : False\n",
      "nombre de dates attendues : 149016\n",
      "nombre de valeurs réel : 149016\n"
     ]
    }
   ],
   "source": [
    "# Teste si toutes les valeurs sont présentes\n",
    "Liste = sorted(df[\"timestamp\"].tolist())\n",
    "i=0\n",
    "booleen = False\n",
    "while i < len(Liste)-1 and not booleen:\n",
    "    if Liste[i] == Liste[i+1]:\n",
    "        print(Liste[i])\n",
    "        booleen = True\n",
    "    i+=1\n",
    "print(\"Il y a des doublons dans les dates : \"+str(booleen))\n",
    "#On calcule le nombre de dates attendues, \n",
    "#comme on sait qu'il n'y a pas de doublons, \n",
    "# si la taille de la liste est le nombre de valeurs attendues, \n",
    "# alors il ne manque aucune données\n",
    "j_par_mois = [31,28,31,30,31,30,31,31,30,31,30,31]\n",
    "nbr_29_fevrier = 4 #nombre d'années bissextiles entre 2005 et 2022\n",
    "nbr_valeurs_attendu = (sum(j_par_mois)*(2022-2005)+nbr_29_fevrier)*24\n",
    "\n",
    "print(f\"nombre de dates attendues : {nbr_valeurs_attendu}\\nnombre de valeurs réel : {len(Liste)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31748.0\n",
      "45.0\n"
     ]
    }
   ],
   "source": [
    "print(df[\"export_france\"].max())\n",
    "print(df[\"export_france\"].min())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math as m\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales\n",
    "\n",
    "h = 10\n",
    "nepochs = 1\n",
    "dn = 50000.\n",
    "nout = 1\n",
    "ker = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranformer les données en dataloader\n",
    "\n",
    "exports = np.array(df[\"export_france\"].tolist())/dn\n",
    "#exports = torch.Tensor(exports).view(24, -1)\n",
    "\n",
    "# On sépare les données \n",
    "#          train : 2005-2017\n",
    "#          validation : 2018-2019\n",
    "#          test : 2020-2021\n",
    "\n",
    "train_nbr = (sum(j_par_mois)*(2018-2005)+nbr_29_fevrier-1)*24\n",
    "\n",
    "validation_nbr = train_nbr +  (sum(j_par_mois)*(2020-2018))*24\n",
    "test_nbr = validation_nbr + (sum(j_par_mois)*(2022-2020)+1)*24\n",
    "\n",
    "\n",
    "train = exports[:train_nbr]\n",
    "validation = exports[train_nbr:validation_nbr]\n",
    "test = exports[validation_nbr:]\n",
    "\n",
    "def xety(data) :\n",
    "    data_jour = torch.Tensor(data).view(-1,24)\n",
    "    x = data_jour[:-1].view(1, 24, -1)\n",
    "    y = data_jour[7:].view(1, 24, -1)\n",
    "    return x,y\n",
    "\n",
    "trainx,trainy = xety(train)\n",
    "validx,validy = xety(validation)\n",
    "testx,testy = xety(test)\n",
    "\n",
    "# trainx = 1, seqlen, 1\n",
    "# trainy = 1, seqlen, 1\n",
    "trainds = torch.utils.data.TensorDataset(trainx, trainy)\n",
    "trainloader = torch.utils.data.DataLoader(trainds, batch_size=1, shuffle=False)\n",
    "validds = torch.utils.data.TensorDataset(validx, validy)\n",
    "validloader = torch.utils.data.DataLoader(validds, batch_size=1, shuffle=False)\n",
    "testds = torch.utils.data.TensorDataset(testx, testy)\n",
    "testloader = torch.utils.data.DataLoader(testds, batch_size=1, shuffle=False)\n",
    "crit = nn.MSELoss()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexio/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([1, 24, 724])) that is different to the input size (torch.Size([24, 724])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/alexio/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([1, 24, 4741])) that is different to the input size (torch.Size([24, 4741])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n",
      "torch.Size([1, 24, 730])\n",
      "torch.Size([1, 24, 4747])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "class Mod(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mod, self).__init__()\n",
    "        self.cnn = nn.Sequential(nn.Conv1d(24,24, 3, stride=1),\n",
    "                                 nn.Sigmoid(),\n",
    "                                 nn.Conv1d(24, 24, 3, stride=1),\n",
    "                                 nn.Sigmoid(),\n",
    "                                 nn.Conv1d(24, 24, 3, stride=1),\n",
    "                                 nn.Sigmoid())\n",
    "        self.rnn = nn.Sequential(nn.RNN(24, 24, num_layers=3),\n",
    "                                 nn.Sigmoid())\n",
    "    def forward(self, x):\n",
    "        # x.shape = (1,351,7) -> (N,Hin,L)\n",
    "        # cnn needs (N,L,Hin) (B,D,T)(batch,time,dim)\n",
    "        print(x.shape)\n",
    "        # 1 724 1 \n",
    "        xx = x.view(1,24,-1)\n",
    "        y = self.cnn(xx)\n",
    "        # y.shape = (1,1,346)\n",
    "        return y.squeeze(0).squeeze(0)\n",
    "\n",
    "\n",
    "def test(mod):\n",
    "    mod.train(False)\n",
    "    totloss, nbatch = 0., 0\n",
    "    for data in testloader:\n",
    "        inputs, goldy = data\n",
    "        haty = mod(inputs)\n",
    "        loss = crit(haty, goldy)\n",
    "        totloss += loss.item()\n",
    "        nbatch += 1\n",
    "    totloss /= float(nbatch)\n",
    "    mod.train(True)\n",
    "    return totloss\n",
    "\n",
    "def train(mod, nepochs, lr):\n",
    "    optim = torch.optim.Adam(mod.parameters(), lr=lr)\n",
    "    tab_train_loss = []\n",
    "    tab_test_loss = []\n",
    "    for epoch in range(nepochs):\n",
    "        testloss = test(mod)\n",
    "        totloss, nbatch = 0., 0\n",
    "        for data in trainloader:\n",
    "            inputs, goldy = data\n",
    "            optim.zero_grad()\n",
    "            haty = mod(inputs)\n",
    "            loss = crit(haty, goldy)\n",
    "            totloss += loss.item()\n",
    "            nbatch += 1\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        totloss /= float(nbatch)\n",
    "        tab_train_loss.append(totloss)\n",
    "        tab_test_loss.append(testloss)\n",
    "        # print(\"err\", totloss, testloss)\n",
    "    # print(\"fin\", totloss, testloss, file=sys.stderr)\n",
    "    return tab_train_loss, tab_test_loss\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tab_value = []\n",
    "    tab_loss = []\n",
    "    n_epochs = 100\n",
    "    model = Mod()\n",
    "    a,b = train(model,n_epochs,lr=0.001)\n",
    "    plt.plot(a,range(len(a)))\n",
    "    plt.plot(b,range(len(b)))\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 33, 4])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Conv1d(16, 33, 3, stride=2)\n",
    "input = torch.randn(20, 16, 10)\n",
    "output = m(input)\n",
    "print(output.shape)\n",
    "\n",
    "24,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
